# Haskell AI Consistency Study Configuration
# Testing AI model consistency in generating functionally equivalent Haskell code

name: "Haskell AI Consistency Study - Basic vs Advanced"
description: "Testing consistency of AI code generation across different models for Haskell implementations"

# AI Models to test
models:
  - id: "claude-3.5-sonnet"
    name: "Claude 3.5 Sonnet"
    provider: "anthropic"
    modelName: "claude-3-5-sonnet-20241022"
    temperature: 0.1
    maxTokens: 2000
    
  - id: "gpt-4-turbo"
    name: "GPT-4 Turbo"
    provider: "openai"
    modelName: "gpt-4-turbo-preview"
    temperature: 0.1
    maxTokens: 2000

# Instruction sets for code generation
instructions:
  - id: "basic-module"
    name: "Basic Haskell Module"
    filePath: "./instructions/haskell-basic-module.md"
    content: ""
    category: "simple"
    description: "Test consistency on simple list operations with Maybe types"
    
  - id: "advanced-types"
    name: "Advanced Haskell Types"
    filePath: "./instructions/haskell-advanced-types.md"
    content: ""
    category: "modern"
    description: "Test consistency on algebraic data types and type classes"

# Study parameters
runsPerCombination: 12  # Number of runs for each model × instruction combination (48 total runs)
outputDir: "./results/haskell-study"
timeout: 30000  # 30 seconds per generation

# Safety configuration
# Automatic random delays between generations to avoid automated usage detection
# This makes the study pattern look like natural human development workflow
safetyConfig:
  enableDelays: true
  minDelaySeconds: 60    # 1 minute minimum
  maxDelaySeconds: 180   # 3 minutes maximum
  delayDescription: "Random delays between 1-3 minutes to simulate human thinking time"

# Study metadata
studyMetadata:
  objective: "Measure consistency of AI models in generating functionally equivalent Haskell code"
  hypothesis: "Different AI models will show varying levels of consistency, with some preferring different implementation patterns"
  
  # Metrics to evaluate
  metrics:
    - "Syntactic similarity (AST structure)"
    - "Semantic equivalence (behavior)"
    - "Code style consistency"  
    - "Error handling patterns"
    - "Type signature consistency"
    - "Documentation quality"
    
  # Expected study characteristics
  expectedDuration: "90-120 minutes"  # ~2 hours with 1-3 min safety delays
  expectedFiles: 48  # 2 models × 2 instructions × 12 runs
  
  # Research questions
  researchQuestions:
    - "Do different AI models generate structurally similar Haskell code?"
    - "Is there consistency in error handling approach (Maybe vs Either vs exceptions)?"
    - "Do models consistently use modern Haskell features vs basic patterns?"
    - "How much variation exists in type signature design?"
    
  # Success criteria
  successCriteria:
    minSyntacticSimilarity: 0.7  # 70% AST similarity within model
    minSemanticEquivalence: 0.9  # 90% behavioral equivalence
    maxCoefficientOfVariation: 0.3  # Consistency measure 